# train_ai.py

import pygame
pygame.init()
pygame.display.set_mode((1, 1), pygame.HIDDEN)

import os
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.env_checker import check_env
from main import GymRacerEnv
import torch
torch.set_num_threads(torch.get_num_threads())
torch.set_num_threads(8)  #–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —è–¥–µ—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–ª—è –æ–±—á—É–µ–Ω–∏—è

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
track_path = "tracks/track_01.json" #—Ç—Ä–µ–∫ –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º —Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è
model_save_dir = "./models/"
os.makedirs(model_save_dir, exist_ok=True)

# === –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã ===
env = GymRacerEnv(track_path)
print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ä–µ–¥—ã...")
check_env(env, warn=True)
print("‚úÖ –°—Ä–µ–¥–∞ –ø—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É!")

checkpoint_callback = CheckpointCallback(
    save_freq=25_000, # —Ä–∞–∑ –≤ —Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –ò–ò —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è
    save_path=model_save_dir,
    name_prefix="racer_model",
    save_replay_buffer=False,
    save_vecnormalize=False
)

# === –ú–æ–¥–µ–ª—å ===
model = PPO(
    "MlpPolicy",
    env,
    verbose=1,
    tensorboard_log="./logs/",
    learning_rate=3e-4,
    n_steps=4096,
    batch_size=128,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    ent_coef=0.03,
)

# === –û–±—É—á–µ–Ω–∏–µ  ===
print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è")
model.learn(
    total_timesteps=2_000_000, #–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    callback=checkpoint_callback,
    progress_bar=True,
    tb_log_name="racer_run"
)

# === –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
model.save(os.path.join(model_save_dir, "final_model"))
print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")